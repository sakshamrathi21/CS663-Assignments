\documentclass[a4paper,12pt]{article}
\usepackage{xcolor}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{array}
\usetikzlibrary{shadows}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{lipsum}
\usepackage{mdframed}
\usepackage{pagecolor}
\usepackage{mathpazo}   % Palatino font (serif)
\usepackage{microtype}  % Better typography

% Page background color
\pagecolor{gray!10!white}

% Geometry settings
\geometry{margin=0.5in}
\pagestyle{fancy}
\fancyhf{}

% Fancy header and footer
\fancyhead[C]{\textbf{\color{blue!80}CS663 Assignment-4}}
% \fancyhead[R]{\color{blue!80}Saksham Rathi}
\fancyfoot[C]{\thepage}

% Custom Section Color and Format with Sans-serif font
\titleformat{\section}
{\sffamily\color{purple!90!black}\normalfont\Large\bfseries}
{\thesection}{1em}{}

% Custom subsection format
\titleformat{\subsection}
{\sffamily\color{cyan!80!black}\normalfont\large\bfseries}
{\thesubsection}{1em}{}

% Stylish Title with TikZ (Enhanced with gradient)
\newcommand{\cooltitle}[1]{%
  \begin{tikzpicture}
    \node[fill=blue!20,rounded corners=10pt,inner sep=12pt, drop shadow, top color=blue!50, bottom color=blue!30] (box)
    {\Huge \bfseries \color{black} #1};
  \end{tikzpicture}
}
\usepackage{float} % Add this package

\newenvironment{solution}[2][]{%
    \begin{mdframed}[linecolor=blue!70!black, linewidth=2pt, roundcorner=10pt, backgroundcolor=yellow!10!white, skipabove=12pt, skipbelow=12pt]%
        \textbf{\large #2}
        \par\noindent\rule{\textwidth}{0.4pt}
}{
    \end{mdframed}
}

% Document title
\title{\cooltitle{CS663 Assignment-4}}
\author{{\bf Saksham Rathi, Kavya Gupta, Shravan Srinivasa Raghavan} \\
\small Department of Computer Science, \\
Indian Institute of Technology Bombay \\}
\date{}

\newtheorem{Def}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\begin{document}
\maketitle

\section*{Question 3}

\begin{solution}{Solution}
    Let $A$ be a real $m \times n$ matrix. $A$ can always be expressed as $A = U \Sigma V^{T}$ where 
    $U \in \mathbb{R}^{m \times m}$, $V \in \mathbb{R}^{n \times n}$ and $\Sigma \in \mathbb{R}^{m \times n}$, 
    $U,V$ being orthogonal matrices and $\Sigma$ being a diagonal matrix with non negative values (called singular values)
    on the diagonal. Let $\Sigma = diag(\sigma_{1},\sigma_{2}, \dots,\sigma_{\min(m,n)})$. We will show that the squares 
    of the non-zero singular values of $A$ are the eigenvalues of either $AA^{T}$ or $A^{T}A$. This is equivalent to 
    showing that the the non-zero singular values of $A$ are the squareroots of either $AA^{T}$ or $A^{T}A$ because,
    the singular values are non-negative by definition. 
    \section*{Part a}
      We will show that the non zero singular values of $A$ are equal to the square roots of the eigen values of either 
      $AA^{T}$ or $A^{T}A$. We define $\Sigma_{m}^{2} = \Sigma \Sigma^{T}$ and $\Sigma^{2}_{n} = \Sigma^{T} \Sigma$.

      \begin{align*}
        AA^{T} &= (U \Sigma V^{T}) \cdot {(U \Sigma V^{T})}^{T} \\
               &= U \Sigma V^{T} \cdot (V \Sigma^{T} U^{T}) \\
               &= U \Sigma V^{T} V \Sigma^{T} U^{T} \\
               &= U \Sigma (V^{T}V) \Sigma^{T} U^{T} \\ 
               &= U \Sigma I_{n} \Sigma ^{T} U^{T} \\
               &= U \Sigma \Sigma^{T} U^{T} \\
               &= U \Sigma^{2}_{m} U^{T} \\
      \end{align*}

      Similarly, $A^{T}A = V \Sigma^{T} \Sigma V^{T} = V \Sigma_{n}^{2} V^{T}$.

      Clearly, $\Sigma^{2}_{m} \in R^{m \times m}$ and equals $diag(\sigma_{1}^{2},\sigma_{2}^{2},\dots , \sigma_{m}^{2}) = D$.
      Now $m = \min(m,n)$ or $n = \min(m,n)$.  

      Let $m = \min(m,n)$. In this case, $\Sigma_{m}^{2}$ is a diagonal matrix ($D$) whose entries are squares of the singular 
      values of $A$. Since $AA^{T}$ is a real symmetric matrix, by \textbf{spectral theorem}  it has an orthogonal decomposition given by 
      $AA^{T} = \mathcal{U} \mathcal{D} \mathcal{U^{T}}$ where $\mathcal{D}$ is a diagonal matrix whose entries are the eigenvalues
      of $AA^{T}$ and $\mathcal{U}$ is an orthogonal matrix. Therefore the non zero entries of $D$ are the eigenvalues of $AA^{T}$.
      Since $D = \Sigma_{m}^{2}$, the non zero entries of $D$ are the squares of the non zero singular values of $A$ and the squares of the any 
      non-zero singular value of $A$ is an eigenvalue of $AA^{T}$, we are done.

      Let $n = \min(m,n)$. In this case, we deal with the diagonal matrix $D = \Sigma_{n}^{2}$ whose entries are the squares of 
      all the singular values of $A$ (this is because $n = \min(m,n)$). The proof is very similar to the case above. Since
      $A^{T}A$ is a real symmetric matrix, it has an orthogonal decomposition $\mathcal{V} \mathcal{D} \mathcal{V}^{T}$
      and therefore we have the non-zero entries of $D$ to be the eigenvalues of $A^{T}A$ and following a similar argument we 
      arrive at the conclusion that the square of any non-zero singular value of $A$ is an eigen value of $A^{T}A$ and that
      any eigenvalue of $A^{T}A$ is the square of some non-zero singular value of $A$. This completes the proof.
      
      \section*{Part b}

      The Frobenius norm of a matrix $A \in \mathbb{R}^{m \times n}$ is defined as 
      \[\left \lvert \lvert A \right \rvert \rvert_{F} = \sqrt{\left(\sum\limits_{i = 1}^{m} \sum\limits_{j = 1}^{n} A_{ij}^{2}\right)} \]

      For any matrix $A \in \mathbb{R}^{m \times n}$ we have 
      \[ \sum\limits_{i = 1}^{m}\sum\limits_{j = 1}^{n} A_{ij}^{2} = \text{Tr}(AA^{T}) = \text{Tr}(A^{T}A)\] where $\text{Tr}(M) = \sum\limits_{i = 1}^{n} M_{ii}$
      for any $n \times n$ square matrix $M$. The trace of a matrix also has the following property,
      \[\text{Tr}(AB) = \text{Tr}(BA)\] whenever $AB$ and $BA$ are both square matrices for two matrices $A$ and $B$.

      WLOG we take $m = \min(n,m)$ (in the other case we simply deal with $A^{T}A$ and $\Sigma_{n}^{2}$)
      \begin{align*}
          \lvert\lvert A \rvert\rvert_{F}^{2} &= \sum\limits_{i = 1}^{m}\sum\limits_{j = 1}^{n} A_{ij}^{2} \\
                                              &= \text{Tr}(AA^{T}) \\
                                              &= \text{Tr}((U\Sigma V^{T} V \Sigma^{T} U^{T})) \\
                                              &= \text{Tr} (U \Sigma_{m}^{2} U^{T}) \\
                                              &= \text{Tr} ((U \Sigma_{m}^{2}) (U^{T})) \\
                                              &= \text{Tr} ((U^{T})(U \Sigma_{m}^{2})) \\
                                              &= \text{Tr} (U^{T} U \Sigma_{m}^{2}) \\
                                              &= \text{Tr} (I_{m} \Sigma_{m}^{2}) \\
                                              &= \text{Tr} (\Sigma_{m}^{2}) \\
                                              &= \sum\limits_{i = 1}^{k} \sigma_{i}^{2} \,\text{ (where k is the number of non-zero singular values of A)} \\
          \Rightarrow \lvert\lvert A \rvert \rvert_{F}^{2} &= \sum\limits_{i = 1}^{k} \sigma_{i}^{2} 
      \end{align*}

      Thus we have the square of the Frobenius norm of a matrix to equal the sum of squares of the singular values.

      \section*{Part c}
        When the SVD is computed through the eigenvalue decomposition of $AA^{T}$ and $A^{T}A$, we get the correct singular
        values but $U \Gamma V^{T}$ does not equal $A$. This is because the eigenvectors of $AA^{T}$ and $A^{T}A$ may be 
        of opposite signs with respect to each other. As given in the slides, 
        \[ A = \sum\limits_{i = 1}^{r} \gamma_{i} \vec{u_{i}}\vec{v_{i}}^{T}\]
        where $r$ is the number of non zero singular values of $A$ and $\vec{u_{i}}$ and $\vec{v_{i}}$ are the corresponding eigenvectors
        (ie the eigenvectors of $AA^{T}$ and those of $A^{T}A$ respectively with eigenvalue equal to $\gamma_{i}^{2}$).
        
        In part d of this question, $\vec{u_{i}}$ is defined such that $A\vec{u_{i}} = \gamma_{i} \vec{v_{i}}$. However the eigenvalue
        decomposition may return $-\vec{u_{i}}$ and $\vec{v_{i}}$ or $\vec{u_{i}}$ and $-\vec{v_{i}}$. This results in the product 
        $\vec{u_{i}}\vec{v_{i}}^{T}$ having a flipped sign from the one given in the summation above. Therefore the results obtained
        from the eigval routine gives the correct singular values but not the correct singular value decomposition because of the sign
        mismatch between corresponding left and right signular vectors.

        Now this can be fixed by computing the product  $\vec{u_{i}}^{T} \cdot (A^{T}\vec{v_{i}})$. If the signs matched then the overall
        product will have a positive value (check lemma\@\ref{l3} for defition of $\vec{u_{i}}$). Whenever the product 
        is negative flip the signs of the vector $\vec{v_{i}}$. Now construct the matrix $U$ and $V$ with the corrected columns.
        This will yield the correct SVD of any matrix $A$.
        
      \section*{Part d Singular Value Decomposition}

      Given:
      \begin{align*}
        A \in \mathbb{R}^{m \times n} \, &, m \leqslant n \\
        P &= A^{T}A \\
        Q &= AA^{T}
      \end{align*}

      A few results and definitions we will use:
      \begin{Def}\label{psd}
         A matrix $M \in \mathbb{R}^{n \times n}$ is said to be positive semi-definite if $\forall \vec{x} \in R^{n}$,
         \[ \vec{v}^{T} M \vec{v} \geqslant 0\]
      \end{Def}
      \begin{theorem}[Spectral Theorem]\label{spectral}
        Any real symmetric matrix $A \in \mathbb{R}^{n \times n}$ is orthogonally diagonalizable. That is there exists real numbers
        $\lambda_{1},\lambda_{2}, \dots,\lambda_{n}$, $U \in \mathbb{R}^{n \times n}$ such that $A = UDU^{T}$ where 
        $D = \text{diag}(\lambda_{1},\lambda_{2},\dots,\lambda_{n})$,
        $U = \left[ \vec{u}_{1} \vec{u}_{2} \cdots \vec{u}_{n}\right]$ with 
        $A \vec{u}_{i} = \lambda_{i} \vec{u}_{i} \, \forall i \in [1,n]$ and $UU^{T} = U^{T}U = I_{n}$
      \end{theorem}
        \begin{lemma}\label{l1}
        $P$ and $Q$ are positive semi-definite (defintion \@\ref{psd}) and their eigenvalues are non-negative.
          \begin{proof}
            We will first show that $P$ is positive semi-definite and then we will show that the eigen values of $P$ 
            are all non-negative. The proof for $Q$ is similar.     
            
            Let $\vec{x} \in \mathbb{R}^{n}$. 
            \begin{align}
            \vec{x}^{T} P \vec{x} &= \vec{x}^{T}(A^{T}A) \vec{x} \nonumber \\ 
                                  &= \vec{x}^{T} A^{T} A \vec{x} \nonumber \\
                                  &= (\vec{x}^{T} A^{T}) (A \vec{x}) \nonumber \\
                                  &= {(A \vec{x})}^{T} (A\vec{x}) \nonumber \\
                                  &= \vec{y}^{T} \vec{y} \, \text{ where $\vec{y} \in \mathbb{R}^{n} \, \vec{y} = A \vec{x}$} \nonumber \\
                                  &= \lvert \lvert \vec{y} \rvert \rvert^{2} \geqslant 0 \nonumber \\
            \Rightarrow \label{r1}\vec{x}^{T} P \vec{x} &\geqslant 0
            \end{align}
            
            Let $\vec{u} \in R^{n}/\{\vec{0}\} $ be an eigenvector of $P$ with corresponding eigenvalue $\lambda \in \mathbb{C}$.
            Consider the expression $\vec{u}^{T} P \vec{u}$. From equation \@\ref{r1} we know that $\vec{u}^{T} P \vec{u} \geqslant 0$.
            Therefore we have,

            \begin{align}
               \vec{u}^{T} P \vec{u} &= \vec{u}^{T}(P \vec{u}) \geqslant 0 \nonumber  \\
                                     = \vec{u}^{T}(\lambda \vec{u}) &\geqslant 0 \nonumber \\
                                     = \lambda \vec{u}^{T} \vec{u} &\geqslant 0\nonumber \\
                                     = \lambda \lvert\lvert \vec{u} \rvert\rvert^{2} &\geqslant 0 \nonumber \\
                \Rightarrow \lambda \lvert\lvert \vec{u} \rvert\rvert^{2} &\geqslant 0 \nonumber \\
                \Rightarrow \label{r2}\lambda &\geqslant 0
            \end{align}

            Therefore we have eigenvalues of $P$ to be non-negative. The proofs hold for $Q$ as well, just replace $\vec{x}$ in
            equation \@\ref{r1} by $\vec{y} \in \mathbb{R}^{m}$ and $\vec{u}$ in equation \@\ref{r2} by
            $\vec{v} \in \mathbb{R}^{m}$.
          \end{proof}
        \end{lemma}

        \begin{lemma}\label{l2}The following are true regarding the eigenvectors of $P$ and $Q$:
          \begin{enumerate}
            \item If $\vec{u} \in \mathbb{R}^{n}$ is an eigenvector of $P$ with eigenvalue $\lambda$ then $A\vec{u}$ is an eigenvector of 
            $Q$ with eigenvalue $\lambda$.
            \item If $\vec{v} \in \mathbb{R}^{m}$ is an eigenvector of $Q$ with eigenvalue $\mu$, then $A^{T} \vec{v}$ is an eigenvector of $P$ with eigenvalue $\mu$.
        \end{enumerate}
          \begin{proof}
            Since $\vec{u} \in \mathbb{R}^{n}$ is an eigenvector of $P$ with eigenvalue $\lambda$ we have,
            \begin{align}
              P\vec{u} &= \lambda \vec{u} \nonumber \\
              \Rightarrow P\vec{u} &= (A^{T}A) \vec{u} = \lambda \vec{u} \nonumber \\
              \Rightarrow (A^{T}) (A \vec{u}) &= \lambda \vec{u} \nonumber \\
              \Rightarrow A \left( A^{T} (A\vec{u})\right) &= A \left( \lambda \vec{u}\right) \nonumber \, \text{ (pre-multiplying both LHS and RHS by a non-null matrix $A$)} \nonumber \\
              \Rightarrow  (A A^{T}) (A\vec{u}) &= \lambda (A \vec{u}) \nonumber \\
              \Rightarrow (Q) (A\vec{u}) &= \lambda (A\vec{u}) \nonumber \\
              \Rightarrow  \label{r3}Q (A\vec{u}) &= \lambda (A\vec{u})
            \end{align}
            From equation \@\ref{r3} we have that for every $\vec{u}$ that is eigenvector of $P$, the vector $A\vec{u}$
            is an eigenvector of $Q$ with the same eigenvalue. Now we prove the second part of the lemma whose proof is very
            similar to what we saw above.

            Since $\vec{v} \in \mathbb{R}^{m}$ is an eigenvector of $Q$ with eigenvalue $\mu$ we have,
            \begin{align}
              Q\vec{v} &= \mu \vec{v} \nonumber \\
              \Rightarrow Q\vec{v} &= (AA^{T}) \vec{v} = \mu \vec{v} \nonumber \\
              \Rightarrow (A) (A^{T} \vec{v}) &= \mu \vec{v} \nonumber \\
              \Rightarrow A^{T} \left( A (A^{T}\vec{v})\right) &= A^{T} \left( \mu \vec{v}\right) \nonumber \, \text{ (pre-multiplying both LHS and RHS by a non-null matrix $A^{T}$)} \nonumber \\
              \Rightarrow  (A^{T} A) (A^{T}\vec{v}) &= \mu (A^{T} \vec{v}) \nonumber \\
              \Rightarrow (P) (A^{T}\vec{v}) &= \mu (A^{T}\vec{v}) \nonumber \\
              \Rightarrow  \label{r4}P (A^{T}\vec{v}) &= \mu (A^{T}\vec{v})
            \end{align}

            Equations \@\ref{r3} and \@\ref{r4} complete the proof.
          \end{proof}
        \end{lemma}

        \begin{lemma}\label{l3}
          Let $\vec{v}_{i} \in \mathbb{R}^{m}$ be an eigenvector of $Q$. Define 
          $u_{i} \stackrel{\Delta}{=} \frac{A^{T} \vec{v}_{i}}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}}$. There exists 
          $\gamma_{i} \geqslant 0$ such that $A\vec{u_{i}} = \gamma_{i} \vec{v}_{i}$. Also, $\vec{u_{i}}$ is a unit eigenvector of $P$.
          If $\vec{v_{i}}$ are all orthogonal to each other then $\vec{u_{i}}$ are also orthogonal to each other $\forall i \in [1,m]$.
          \begin{proof} 
            Let $\vec{v}_{i} \in \mathbb{R}^{m}$ be an eigenvector of $Q$ with an eigenvalue $\mu$. From lemma \@\ref{l1} we know that eigenvalues of $P$ and $Q$ are non-negative, therefore we have 
            $\mu_{i} \geqslant 0$.
            Consider the expression $A\vec{u}_{i}$,
            \begin{align}
               A\vec{u}_{i} &= A \left(\frac{A^{T} \vec{v}_{i}}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}}\right) \nonumber \\
                            &= \frac{ A (A^{T} \vec{v}_{i})}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}} \nonumber \\
                            &= \frac{ (A A^{T}) \vec{v}_{i}}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}} \nonumber \\
                            &= \frac{Q\vec{v}_{i}}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}} \nonumber \\
                            &= \frac{\mu_{i}\vec{v}_{i}}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}} \nonumber \\
                            &= \left(\frac{\mu_{i}}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}}\right) \vec{v}_{i} \nonumber \\
                            &= \gamma_{i} \vec{v}_{i}
            \end{align}

            That is, there exists 
            $\gamma_{i} = \left(\frac{\mu_{i}}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}}\right) \geqslant 0$ 
            such that $A\vec{u}_{i} = \gamma_{i} \vec{v}_{i}$, $\vec{u}_{i}$ is a unit vector and by lemma \@\ref{l2}, $\vec{u_{i}}$ is also an eigenvector of $P$.
            
            We will show that $\vec{u_{i}}^{T} \cdot \vec{u_{j}} = 0$, $\forall i \neq j$. $\vec{v_{i}}$ is an eigenvector
            of $Q$ with eigenvalue $\mu_{i} \, \forall i \in [1,m]$.
            \begin{align*}
              \vec{u_{i}}^{T} \cdot \vec{u_{j}} &= \left(\frac{A^{T} \vec{v}_{i}}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}}\right)^{T}
              \cdot \left(\frac{A^{T} \vec{v}_{j}}{\lvert\lvert A^{T} \vec{v}_{j} \rvert\rvert_{2}}\right) \\
              &= \frac{1}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}\cdot  \lvert\lvert A^{T} \vec{v}_{j} \rvert\rvert_{2}} 
              \left( {(A^{T}\vec{v_{i}})}^{T} (A^{T}\vec{v_{j}})\right) \\
              &= \frac{1}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}\cdot  \lvert\lvert A^{T} \vec{v}_{j} \rvert\rvert_{2}} 
              \left( (\vec{v_{i}}^{T} A) (A^{T}\vec{v_{j}})\right) \\              
              &= \frac{1}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}\cdot  \lvert\lvert A^{T} \vec{v}_{j} \rvert\rvert_{2}} 
              \left( (\vec{v_{i}}^{T} (AA^{T}) \vec{v_{j}})\right) \\
              &= \frac{1}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}\cdot  \lvert\lvert A^{T} \vec{v}_{j} \rvert\rvert_{2}} 
              \left( (\vec{v_{i}}^{T} (Q) \vec{v_{j}})\right) \\
              &= \frac{1}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}\cdot  \lvert\lvert A^{T} \vec{v}_{j} \rvert\rvert_{2}} 
              \left( (\vec{v_{i}}^{T} (Q \vec{v_{j}}))\right) \\
              &= \frac{1}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}\cdot  \lvert\lvert A^{T} \vec{v}_{j} \rvert\rvert_{2}} 
              \left( (\vec{v_{i}}^{T} (\mu_{j} \vec{v_{j}}) )\right) \\
              &= \frac{1}{\lvert\lvert A^{T} \vec{v}_{i} \rvert\rvert_{2}\cdot  \lvert\lvert A^{T} \vec{v}_{j} \rvert\rvert_{2}} 
              \left( (\mu_{i}(\vec{v_{i}}^{T} \cdot \vec{v_{j}}))\right) \\
              &= 0 \,\text{ (as $\vec{v_{i}}$ and $\vec{v_{j}}$ are orthogonal to each other)}
            \end{align*}

          \end{proof}
        \end{lemma}
        \begin{lemma}\label{l4}
          Let $\{\vec{v_{i}} | i \in [1,m]\}$, $\{\vec{u_{i}} | i \in [1,n]\}$ be sets of orthonormal eigenvectors of $Q$,$P$ respectively 
          such that $\vec{u_{i}} = \frac{A^{T}\vec{v_{i}}}{\lvert\lvert A^{T}\vec{v_{i}}\rvert\rvert_{2}}$,
          $ \forall i \in [1,m]$. Then,
          \[ A \vec{u_{i}} = \vec{0}\]
          $\forall i \in [m + 1,n]$.
          \begin{proof}
              Clearly the rowspace of $A$ ($\mathcal{R}(A)$) has dimensions at most $m$ since there are only $m$ rows. Therefore the space spanned
              by the row vectors of $A$ can have an orthonormal basis which consists of at most $m$ vectors. 

              Any vector $\vec{x} \in \mathcal{R}(A)$ can be written as $\sum\limits_{i = 1}^{m} c_{i} \vec{a_{i}}$ where 
              $\vec{a_{i}} \in \mathbb{R}^{n}$ are the rows of $A$ and $c_{i} \in \mathbb{R}$. This can also be written as 
              \[\vec{x} = A^{T} \vec{c}\]
              where $\vec{c} = (c_{1},c_{2},\dots c_{m})$. 

              Note that $\vec{u_{i}} = A^{T}\vec{v_{i}^{'}}$ where 
              $\vec{v_{i}^{'}} = \frac{1}{\lvert\lvert A^{T}\vec{v_{i}}\rvert\rvert_{2}}\vec{v_{i}}$, $\forall i \in [1,m]$. Therefore 
              $\vec{u_{i}} \in \mathcal{R}(A) \, , \forall i \in [1,m]$, ie $\{\vec{u_{i}} | i \in [1,m]\}$ form an 
              orthonormal basis of 
              $\mathcal{R}(A)$ ie $\vec{x} = \sum\limits_{i = 1}^{m} c_{i} \vec{u_{i}}$ for any $\vec{x} \in \mathcal{R}$ and 
              any vector $\vec{y} \notin \mathcal{R}(A)$ has $\vec{y}^{T} \vec{u_{i}} = 0$. Since 
              $\vec{u_{j}}$ is orthogonal to $\vec{u_{i}}$ $\forall j \neq i$, $\vec{u_{j}}$ cannot be represented as a linear
              combination of $\vec{u_{i}}$ when $i \neq j$. Therefore $\vec{u_{j}} \notin \mathcal{R}(A) \, \forall j \in [m + 1,n]$
              since there exists no $c_{i}$ such that $\vec{u_{j}} = \sum\limits_{i = 1}^{m} c_{i} \vec{u_{i}}$.
              
              Therefore $\vec{u_{i}}^{T}\cdot\vec{x} = 0 \, \forall i \in [m + 1,n]\, , \forall \vec{x} \in mathcal{R}(A)$ hence 
              $\vec{u_{i}}^{T} \cdot \vec{a_{i}} = 0 \forall i \in [m + 1,n]\, , \forall j \in [1,m]$. This gives 
              
                  \[A\vec{u_{i}} = \vec{0}\]
              where $i \in [m + 1,n]$.
          \end{proof}
        \end{lemma}
        \begin{theorem}[Singular Value Decomposition]
            Let $\{\vec{v_{i}} | i \in [1,m]\}$ be a set of orthonormal eigenvectors of $Q$, $\gamma_{i}$ be 
            defined as in lemma \@\ref{l3}and $\{\vec{u_{i}} | i \in [1,n]\}$ be a set of orthonormal eigenvectors of 
            $P$ defined as in lemma \@\ref{l4}.
            Let $U = \left[\vec{v}_{1} | \vec{v}_{2} | \cdots | \vec{v}_{m}\right]$,
            $V = \left[\vec{u}_{1} | \vec{u}_{2} | \cdots | \vec{u}_{n}\right]$ and 
            $\Gamma =\text{diag}(\gamma_{1},\gamma_{2},\dots,\gamma_{m})$ where $U$ is an $m \times m$, $\Gamma$ is an 
            $m \times n$ matrix and $V$ is an $n \times n$ matrix. Then \[ A = U \Gamma V^{T} \]
            \begin{proof}
              Since the columns of $U$ and $V$ are the orthonormal eigenvectors of $Q$ and $P$ respectively, we have 
              $UU^{T} = U^{T}U = I_{m}$ and $VV^{T} = V^{T}V = I_{n}$. The result follows naturally from the lemmas proven earlier.
              
              Consider the expression $AV$.
              \begin{align}
                AV &= A \left[ \vec{u}_{1} | \vec{u}_{2} | \cdots | \vec{u}_{n}\right] \nonumber \\
                \Rightarrow AV &= \left[ A\vec{u}_{1} | A\vec{u}_{2} | \cdots | A \vec{u}_{n} \right] \nonumber \\
                               &= \left[ \gamma_{1}\vec{v}_{1} | \gamma_{2}\vec{v}_{2} | \cdots | \gamma_{m} \vec{v}_{m} | 
                               \vec{0} | \cdots |\vec{0}\right] \text{ (from lemma \@\ref{l3} and lemma \@\ref{l4})} \nonumber \\
                               &= \left[\vec{v}_{1} | \vec{v}_{2} | \cdots | \vec{v}_{m}\right] \Gamma \text{ (by observation, since the last $n - m$ columns of $\Gamma$ are $\vec{0}$)}\nonumber \\
                               &= U \Gamma \nonumber \\
                \Rightarrow AV &= U \Gamma \nonumber \\
                \Rightarrow A V V^{T} &= U \Gamma V^{T} \nonumber \\
                \Rightarrow A I_{n} &= U \Gamma V^{T} \nonumber \\
                \Rightarrow A &=  U \Gamma V^{T}
              \end{align}
               
              We have thus shown that every matrix $A \in \mathbb{R}^{m \times n}$ has a \textbf{singular value decomposition}.
            \end{proof}
        \end{theorem}
  \end{solution}
\end{document}